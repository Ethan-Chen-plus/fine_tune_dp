{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b348164-f563-4ce5-ac9d-354a15a2fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "'''Train GPT2 model series with DP (w/ parameter-efficient approach LoRA when lora_dim > 0)'''\n",
    "\n",
    "import datasets\n",
    "import dp_transformers\n",
    "import transformers\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from transformers.training_args import ParallelMode\n",
    "from dp_transformers.layers.dp_merged_linear import mark_only_lora_as_trainable\n",
    "from dp_transformers.module_modification import convert_gpt2_attention_to_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f397792e-0481-4285-a21b-0d2da595e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3168532d-51c9-4ed7-b277-9c5bcdbd48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name: str = field(default=\"gpt2\", metadata={\n",
    "        \"help\": \"Model name in HuggingFace, e.g. 'gpt2'\"\n",
    "    })\n",
    "\n",
    "    lora_dim: int = field(default=0, metadata={\n",
    "        \"help\": \"LoRA dimension; 0 means LoRA is disabled\"\n",
    "    })\n",
    "\n",
    "    sequence_len: int = field(default=128, metadata={\n",
    "        \"help\": \"Model sequence length\"\n",
    "    })\n",
    "\n",
    "    lora_dropout: float = field(default=0.0, metadata={\n",
    "        \"help\": \"Dropout probability for LoRA layers\"\n",
    "    })\n",
    "\n",
    "    lora_alpha: int = field(default=32, metadata={\n",
    "        \"help\": \"LoRA attention alpha\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f53a44b-b475-4905-8d9e-07c7998d8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Arguments:\n",
    "    train: dp_transformers.TrainingArguments\n",
    "    privacy: dp_transformers.PrivacyArguments\n",
    "    model: ModelArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a73e94f-313e-40b8-989a-88abc653e554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n",
      "128\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser(description='Model Training Arguments')\n",
    "    \n",
    "    # 添加参数\n",
    "    parser.add_argument('--output_dir', type=str, default='scratch')\n",
    "    parser.add_argument('--model_name', type=str, default='gpt2')\n",
    "    parser.add_argument('--sequence_len', type=int, default=128)\n",
    "    parser.add_argument('--per_device_train_batch_size', type=int, default=32)\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=2)\n",
    "    parser.add_argument('--evaluation_strategy', type=str, default='steps')\n",
    "    parser.add_argument('--eval_steps', type=int, default=45)\n",
    "    parser.add_argument('--log_level', type=str, default='info')\n",
    "    parser.add_argument('--per_device_eval_batch_size', type=int, default=64)\n",
    "    parser.add_argument('--eval_accumulation_steps', type=int, default=1)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--target_epsilon', type=int, default=8)\n",
    "    parser.add_argument('--per_sample_max_grad_norm', type=float, default=1.0)\n",
    "    parser.add_argument('--prediction_loss_only', action='store_true')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.01)\n",
    "    parser.add_argument('--remove_unused_columns', type=bool, default=False)\n",
    "    parser.add_argument('--num_train_epochs', type=int, default=3)\n",
    "    parser.add_argument('--logging_steps', type=int, default=5)\n",
    "    parser.add_argument('--max_grad_norm', type=int, default=0)\n",
    "    parser.add_argument('--lr_scheduler_type', type=str, default='constant')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
    "    parser.add_argument('--disable_tqdm', type=bool, default=False)\n",
    "    parser.add_argument('--dataloader_num_workers', type=int, default=2)\n",
    "    \n",
    "    args = parser.parse_args(argv)\n",
    "    return args\n",
    "\n",
    "# 解析参数\n",
    "args = parse_arguments(['--dataloader_num_workers','2'])\n",
    "# args = parse_arguments()\n",
    "\n",
    "# 打印解析的参数\n",
    "print(args.model_name)\n",
    "print(args.sequence_len)\n",
    "print(args.per_device_train_batch_size)\n",
    "# 其他参数类似...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c632dd4d-ea8d-4e30-a3d8-94ead538bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='scratch',\n",
    "model_name='gpt2',\n",
    "sequence_len='128',\n",
    "\n",
    "per_device_train_batch_size=32,\n",
    "gradient_accumulation_steps=2,\n",
    "evaluation_strategy='steps',\n",
    "eval_steps=45,\n",
    "log_level='info',\n",
    "per_device_eval_batch_size=64,\n",
    "eval_accumulation_steps=1,\n",
    "seed=42,\n",
    "num_train_epochs=3,\n",
    "logging_steps=5,\n",
    "max_grad_norm=0,\n",
    "lr_scheduler_type='constant',\n",
    "learning_rate=1e-4,\n",
    "disable_tqdm=False,\n",
    "dataloader_num_workers=2\n",
    "\n",
    "\n",
    "--target_epsilon 8 \\\n",
    "--per_sample_max_grad_norm 1.0 \\\n",
    "--prediction_loss_only \\\n",
    "--weight_decay 0.01 \\\n",
    "--remove_unused_columns False \\\n",
    "--num_train_epochs 3 \\\n",
    "--logging_steps 5 \\\n",
    "--max_grad_norm 0 \\\n",
    "--lr_scheduler_type constant \\\n",
    "--learning_rate 1e-4 \\\n",
    "--disable_tqdm False \\\n",
    "--dataloader_num_workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ba425b8-25c9-4f85-8cbe-025077a50590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_transformers import TrainingArguments, PrivacyArguments\n",
    "\n",
    "# 创建TrainingArguments对象\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='scratch',\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=45,\n",
    "    log_level='info',\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_accumulation_steps=1,\n",
    "    seed=42,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=5,\n",
    "    max_grad_norm=0,\n",
    "    lr_scheduler_type='constant',\n",
    "    learning_rate=1e-4,\n",
    "    disable_tqdm=False,\n",
    "    dataloader_num_workers=2,\n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "# 创建PrivacyArguments对象\n",
    "privacy_args = PrivacyArguments(\n",
    "    target_epsilon=8,\n",
    "    per_sample_max_grad_norm=1.0,\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "# 创建ModelArguments对象\n",
    "model_args = ModelArguments(\n",
    "    model_name='gpt2',\n",
    "    sequence_len='128',\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db792a1f-bf19-41e9-a4fe-2bd14d1bfa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "106e2c1d-5053-4ffa-aaf5-19162b162e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d109c60d-9153-4e25-acb5-d17293a7a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = train_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72fafc5a-6a16-441c-9f94-1b799cc7e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2023 20:50:05:WARNING:Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {train_args.local_rank}, device: {train_args.device}, n_gpu: {train_args.n_gpu}, \"\n",
    "    f\"distributed training: {bool(train_args.local_rank != -1)}, 16-bits training: {train_args.fp16}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56d9346a-408f-4ae8-9546-0ca0fd51499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2023 20:50:15:INFO:Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=2,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "dry_run=False,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=45,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=scratch/runs/Aug17_20-49-47_iZ2ze4sox357rkb3f1xg03Z,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "output_dir=scratch,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=64,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=scratch,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Training/evaluation parameters {train_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d818dda-4d66-4a4a-a6e4-0fb3a0634c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2023 20:50:22:INFO:Privacy parameters PrivacyArguments(per_sample_max_grad_norm=1.0, noise_multiplier=None, target_epsilon=8, target_delta=None, disable_dp=False)\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Privacy parameters {privacy_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d09020bf-8238-443d-ac41-e058b6e47969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:668] 2023-08-17 20:51:10,737 >> loading configuration file config.json from cache at /kewei-ai/huggingface/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-08-17 20:51:10,738 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2534] 2023-08-17 20:51:10,740 >> loading weights file model.safetensors from cache at /kewei-ai/huggingface/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2023-08-17 20:51:10,750 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2023-08-17 20:51:12,616 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2023-08-17 20:51:12,617 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2023-08-17 20:51:13,089 >> loading configuration file generation_config.json from cache at /kewei-ai/huggingface/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2023-08-17 20:51:13,089 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_args.model_name)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5143f24-a6ec-4750-8e6d-93a69195535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HEAD request to https://huggingface.co/datasets/reddit/resolve/main/README.md timed out, retrying... [1.0]\n",
      "08/17/2023 20:53:16:INFO:HEAD request to https://huggingface.co/datasets/reddit/resolve/main/README.md timed out, retrying... [1.0]\n",
      "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/reddit/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e\n",
      "08/17/2023 20:53:20:INFO:Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/reddit/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "08/17/2023 20:53:20:INFO:Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e\n",
      "08/17/2023 20:53:20:INFO:Loading Dataset info from /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e\n",
      "Found cached dataset reddit (/root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e)\n",
      "08/17/2023 20:53:20:INFO:Found cached dataset reddit (/root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e\n",
      "08/17/2023 20:53:20:INFO:Loading Dataset info from /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-f76b38dfea796d45.arrow and /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-f1b807689c2069ed.arrow\n",
      "08/17/2023 20:53:20:INFO:Loading cached split indices for dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-f76b38dfea796d45.arrow and /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-f1b807689c2069ed.arrow\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataset = datasets.load_dataset('reddit', split=\"train[:500000]\").train_test_split(0.02, seed=train_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f4fff25-34c8-4522-9f94-3ee5170c5994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:502] 2023-08-17 20:53:42,666 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2023-08-17 20:53:43,126 >> loading configuration file config.json from cache at /kewei-ai/huggingface/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-08-17 20:53:43,127 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-08-17 20:53:44,015 >> loading file vocab.json from cache at /kewei-ai/huggingface/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-08-17 20:53:44,016 >> loading file merges.txt from cache at /kewei-ai/huggingface/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-08-17 20:53:44,017 >> loading file tokenizer.json from cache at /kewei-ai/huggingface/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-08-17 20:53:44,017 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-08-17 20:53:44,017 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2023-08-17 20:53:44,018 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2023-08-17 20:53:44,019 >> loading configuration file config.json from cache at /kewei-ai/huggingface/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-08-17 20:53:44,020 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name)\n",
    "tokenizer.pad_token = -100 # Set a dummy pad token we don't use it anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3564e1ff-9db5-49d0-97ce-e702b2b764f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'128'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "073590c9-d306-4f2e-9d12-88f79c617926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00000_of_00008.arrow\n",
      "08/17/2023 20:54:52:INFO:Process #0 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00000_of_00008.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00001_of_00008.arrow\n",
      "08/17/2023 20:54:52:INFO:Process #1 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00001_of_00008.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00002_of_00008.arrow\n",
      "08/17/2023 20:54:52:INFO:Process #2 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00002_of_00008.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00003_of_00008.arrow\n",
      "08/17/2023 20:54:52:INFO:Process #3 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00003_of_00008.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00004_of_00008.arrow\n",
      "08/17/2023 20:54:52:INFO:Process #4 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00004_of_00008.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00005_of_00008.arrow\n",
      "08/17/2023 20:54:52:INFO:Process #5 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00005_of_00008.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00006_of_00008.arrow\n",
      "08/17/2023 20:54:52:INFO:Process #6 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00006_of_00008.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00007_of_00008.arrow\n",
      "08/17/2023 20:54:52:INFO:Process #7 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00007_of_00008.arrow\n",
      "Spawning 8 processes\n",
      "08/17/2023 20:54:52:INFO:Spawning 8 processes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffb483d63114ade8604f6af404e0d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing dataset (num_proc=8):   0%|          | 0/490000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00004_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00003_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00001_of_00008.arrow\n",
      "08/17/2023 20:54:58:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00003_of_00008.arrow\n",
      "08/17/2023 20:54:57:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00004_of_00008.arrow\n",
      "08/17/2023 20:54:58:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00001_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00000_of_00008.arrow\n",
      "08/17/2023 20:54:58:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00000_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00007_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00005_of_00008.arrow\n",
      "08/17/2023 20:54:58:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00005_of_00008.arrow\n",
      "08/17/2023 20:54:58:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00007_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00002_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00006_of_00008.arrow\n",
      "08/17/2023 20:54:58:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00002_of_00008.arrow\n",
      "08/17/2023 20:54:58:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-4c149cb9494486ab_00006_of_00008.arrow\n",
      "Concatenating 8 shards\n",
      "08/17/2023 21:00:13:INFO:Concatenating 8 shards\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00000_of_00008.arrow\n",
      "08/17/2023 21:00:13:INFO:Process #0 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00000_of_00008.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00001_of_00008.arrow\n",
      "08/17/2023 21:00:13:INFO:Process #1 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00001_of_00008.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00002_of_00008.arrow\n",
      "08/17/2023 21:00:13:INFO:Process #2 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00002_of_00008.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00003_of_00008.arrow\n",
      "08/17/2023 21:00:13:INFO:Process #3 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00003_of_00008.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00004_of_00008.arrow\n",
      "08/17/2023 21:00:13:INFO:Process #4 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00004_of_00008.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00005_of_00008.arrow\n",
      "08/17/2023 21:00:13:INFO:Process #5 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00005_of_00008.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00006_of_00008.arrow\n",
      "08/17/2023 21:00:13:INFO:Process #6 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00006_of_00008.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00007_of_00008.arrow\n",
      "08/17/2023 21:00:13:INFO:Process #7 will write at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00007_of_00008.arrow\n",
      "Spawning 8 processes\n",
      "08/17/2023 21:00:13:INFO:Spawning 8 processes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4c2511fc1f4e74be7e72d1cb9fd47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing dataset (num_proc=8):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00006_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00003_of_00008.arrow\n",
      "08/17/2023 21:00:18:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00006_of_00008.arrow\n",
      "08/17/2023 21:00:18:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00003_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00002_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00005_of_00008.arrow\n",
      "08/17/2023 21:00:18:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00002_of_00008.arrow\n",
      "08/17/2023 21:00:18:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00005_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00007_of_00008.arrow\n",
      "08/17/2023 21:00:19:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00007_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00004_of_00008.arrow\n",
      "08/17/2023 21:00:19:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00004_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00001_of_00008.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00000_of_00008.arrow\n",
      "08/17/2023 21:00:19:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00001_of_00008.arrow\n",
      "08/17/2023 21:00:19:INFO:Caching processed dataset at /root/.cache/huggingface/datasets/reddit/default/1.0.0/bd1bf9097540c9101f329c123d12c6c6a042f65e5f0ab7f9bbabb0a54d3c840e/cache-30641ec52f976fc7_00000_of_00008.arrow\n",
      "Concatenating 8 shards\n",
      "08/17/2023 21:00:21:INFO:Concatenating 8 shards\n"
     ]
    }
   ],
   "source": [
    "# Tokenize data\n",
    "with train_args.main_process_first(desc=\"tokenizing dataset\"):\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: tokenizer(batch['content'], padding=\"max_length\", truncation=True, max_length=int(model_args.sequence_len)),\n",
    "        batched=True, num_proc=8, desc=\"tokenizing dataset\", remove_columns=dataset.column_names['train']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f375d1f-32a5-47a3-a106-c321a5ca2f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "--lora_dim 4 \\\n",
    "--lora_alpha 32 \\\n",
    "--lora_dropout 0.0 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a0d94a1-a5b4-4f42-ba5b-469f06dd6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.lora_dim=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e443c79c-3192-4533-a4eb-427c59b47525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.lora_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98235899-3bde-4081-8cfd-f19586c54ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.lora_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53cf2a72-1345-4da8-9569-6968f70a23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model=model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d531d59-c890-46f4-91bf-42c940e79c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/LLM/lib/python3.11/site-packages/dp_transformers/module_modification.py:19: UserWarning: It looks like you have a model with a classification or LM head. If this is the case, pass `model.transformer` to `convert_gpt2_attention_to_lora` to avoid this warning. \n",
      "  warnings.warn(\"\"\"It looks like you have a model with a classification or LM head. \"\"\"\n"
     ]
    }
   ],
   "source": [
    "if args.model.lora_dim > 0:\n",
    "    model = convert_gpt2_attention_to_lora(\n",
    "        model, r=args.model.lora_dim, lora_alpha=args.model.lora_alpha, lora_dropout=args.model.lora_dropout,\n",
    "        enable_lora=[True, False, True], merge_weights=False\n",
    "    )\n",
    "    mark_only_lora_as_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99744f5f-a7be-4b46-af7f-cc9eeb9c3350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): DPMergedLinear(\n",
       "            (linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "            (lora_B): Conv1DZeroInit()\n",
       "          )\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if train_args.local_rank == 0:\n",
    "    logger.info(f\"Total number of parameters of the model: {model.num_parameters(only_trainable=False)}\")\n",
    "    logger.info(f\"Fine-tuned number of parameters of the model: {model.num_parameters(only_trainable=True)}\")\n",
    "\n",
    "model = model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b1a503f-f0ec-4858-9950-ed1ea5196dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.model.lora_dim > 0:\n",
    "    from dp_transformers.grad_sample.lora import lora_layer\n",
    "else:\n",
    "    from dp_transformers.grad_sample.transformers import conv_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62884603-67bb-401d-8c7d-ee7e52e48d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': -100}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce20a0d4-661c-4a4f-a345-624a1fc8c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = dp_transformers.DataCollatorForPrivateCausalLanguageModeling(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9a41614-f77c-44df-8954-f2bcfa6a7d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForPrivateCausalLanguageModeling(tokenizer=GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': -100}, clean_up_tokenization_spaces=True), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a392134-af2b-4d5f-8db3-23e21cce0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = dp_transformers.dp_utils.OpacusDPTrainer(\n",
    "    args=train_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    data_collator=data_collator,\n",
    "    privacy_args=privacy_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a455c20-7e70-4563-80ba-31d812fb7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:762] 2023-08-17 22:43:40,796 >> The following columns in the training set don't have a corresponding argument in `GradSampleModule.forward` and have been ignored: attention_mask, input_ids. If attention_mask, input_ids are not expected by `GradSampleModule.forward`,  you can safely ignore this message.\n",
      "/root/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1769] 2023-08-17 22:43:40,817 >> ***** Running training *****\n",
      "[INFO|trainer.py:1770] 2023-08-17 22:43:40,818 >>   Num examples = 0\n",
      "[INFO|trainer.py:1771] 2023-08-17 22:43:40,818 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1772] 2023-08-17 22:43:40,819 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1773] 2023-08-17 22:43:40,819 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:1774] 2023-08-17 22:43:40,819 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1775] 2023-08-17 22:43:40,820 >>   Total optimization steps = 22,968\n",
      "[INFO|trainer.py:1776] 2023-08-17 22:43:40,821 >>   Number of trainable parameters = 147,456\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2807, in __getitems__\n    batch = self.__getitem__(keys)\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\n    return self._getitem(key)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\n    _check_valid_index_key(key, size)\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\nIndexError: Invalid key: 419297 is out of bounds for size 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     eps_prv \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mget_prv_epsilon()\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/trainer.py:1899\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1896\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1899\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1900\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2807, in __getitems__\n    batch = self.__getitem__(keys)\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\n    return self._getitem(key)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\n    _check_valid_index_key(key, size)\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 536, in _check_valid_index_key\n    _check_valid_index_key(int(max(key)), size=size)\n  File \"/root/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\nIndexError: Invalid key: 419297 is out of bounds for size 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "finally:\n",
    "    eps_prv = trainer.get_prv_epsilon()\n",
    "    eps_rdp = trainer.get_rdp_epsilon()\n",
    "    trainer.log({\n",
    "        \"final_epsilon_prv\": eps_prv,\n",
    "        \"final_epsilon_rdp\": eps_rdp\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad8c23-6110-4c16-a746-a80821428909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
